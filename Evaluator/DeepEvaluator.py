from deepeval import evaluate
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric
from openai import BadRequestError

# Map metric names to their corresponding classes
METRIC_MAP = {
    "answer_relevancy": AnswerRelevancyMetric,
    "faithfulness": FaithfulnessMetric
}
import os
from Evaluator import Evaluator

output_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..','..','outputs'))

class DeepEval(Evaluator):
    def __init__(self, metric="answer_relevancy"):
        """
        Initialize the DeepEval evaluator with a specific metric.

        Parameters:
            metric (str): The name of the evaluation metric to use. 
                          Must be a key in METRIC_MAP (e.g., "answer_relevancy" or "faithfulness").
        """
        # Instantiate the selected metric with the specified model
        self.metric = METRIC_MAP[metric](model="gpt-4o")  # DeepEval metric
        self.metric_name = metric

    async def evaluate(self, output_prompt, input_prompt=None, context=None, **kwargs):
        """
        Evaluate the output of an LLM using the selected DeepEval metric.

        Parameters:
            output_prompt (str): The actual output generated by the LLM to be evaluated.
            input_prompt (str, optional): The input prompt given to the LLM.
            context (list or str, optional): Additional retrieval context for evaluation. 
                                             Can be a string or a list of strings.
            **kwargs: Additional keyword arguments (not used).

        Returns:
            tuple: 
                - eval_dict (dict): Contains the DeepEval score and reason for the evaluation.
                - warnings (dict): Contains a warning flag if the score is below 0.8.

        Notes:
            - All paths, if any, should be absolute.
            - The function uses DeepEval's LLMTestCase and metric classes to perform evaluation.
        """
        # Instantiate the metric for each evaluation (ensures statelessness)
        metric = METRIC_MAP[self.metric_name]()

        # Ensure context is a list of strings or None
        if context is not None and not isinstance(context, list):
            context = [context] if isinstance(context, str) else []

        # Create test case using DeepEval's structure
        test_case = LLMTestCase(
            input=input_prompt,
            actual_output=output_prompt,
            retrieval_context=context
        )
        try:
        # Measure the metric asynchronously
            await metric.a_measure(test_case)

            # Prepare return dictionaries with score and reason
            eval_dict = {
                f"{self.metric_name}-DeepEval-score": metric.score,
                f"{self.metric_name}-DeepEval-reason": metric.reason
            }

            # Add a warning if the score is below the threshold (0.8)
            warnings = {
                f"{self.metric_name}-DeepEval-warning": metric.score < 0.8  # Threshold at 80%
            }
        except Exception as e:
            eval_dict = {
                f"{self.metric_name}-DeepEval-score": 1.0,
                f"{self.metric_name}-DeepEval-reason": "BadRequest"
            }
            warnings = {
                f"{self.metric_name}-DeepEval-warning": True
            }
            print(f"OUTPUT: {output_prompt}")
            print(f"INPUT: {input_prompt}")
            print(f"CONTEXT: {context}")

            os.makedirs(os.path.join(output_path,"ERRORS"), exist_ok=True)
            with open(os.path.join(output_path,"ERRORS" ,"errors.txt"), mode="w", encoding='utf-8') as f:
                f.write(f"BadRequestError: {e}\n")
                f.write(f"OUTPUT: {output_prompt}\n")
                f.write(f"INPUT: {input_prompt}\n")
                f.write(f"CONTEXT: {context}\n")

        return eval_dict, warnings
